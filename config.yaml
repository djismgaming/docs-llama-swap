# Llama-Swap config and run
# C:\Users\Ismael\apps\llama-swap\llama-swap.exe -watch-config -config c:\Users\Ismael\code\llama-swap\config.yaml

metricsMaxInMemory: 100 # maximum records for metrics
macros:
  "llamacpp-vulkan": >
    C:\Users\Ismael\apps\llamacpp-vulkan\llama-server.exe
    --port ${PORT}
    --no-mmap

  "llamacpp-rocm": >
    C:\Users\Ismael\apps\llamacpp-rocm\llama-server.exe
    --port ${PORT}
    --no-mmap


models:
  GPT-OSS 20B Vulkan:
    ttl: 900 # seconds to unload the model after use
    cmd: | # support for multi-line
      ${llamacpp-vulkan}
      --grammar-file C:\Users\Ismael\code\docs-llama-swap\gpt-oss.gbnf
      -m F:\ai\lmstudio-models\lmstudio-community\gpt-oss-20b-GGUF\gpt-oss-20b-MXFP4.gguf
      --jinja
      --temp 1.0
      --top-p 1.0

  GPT-OSS 20B ROCm:
    ttl: 900 # seconds to unload the model after use
    cmd: | # support for multi-line
      ${llamacpp-rocm}
      --grammar-file C:\Users\Ismael\code\docs-llama-swap\gpt-oss.gbnf
      -m F:\ai\lmstudio-models\lmstudio-community\gpt-oss-20b-GGUF\gpt-oss-20b-MXFP4.gguf
      --jinja
      --temp 1.0
      --top-p 1.0
      

  Qwen3 Coder 30B A3B Q4_K_M Vulkan:
    ttl: 900 # seconds to unload the model after use
    cmd: | # support for multi-line
      ${llamacpp-vulkan}
      -m F:\ai\lmstudio-models\lmstudio-community\Qwen3-Coder-30B-A3B-Instruct-GGUF\Qwen3-Coder-30B-A3B-Instruct-Q4_K_M.gguf
      --jinja
      --temp 0.7
      --min-p 0.0
      --top-p 0.80
      --top-k 20
      --repeat-penalty 1.05
      --fit-ctx 60000

  Qwen3 Coder 30B A3B Q4_K_M ROCm:
    ttl: 900 # seconds to unload the model after use
    cmd: | # support for multi-line
      ${llamacpp-rocm}
      -m F:\ai\lmstudio-models\lmstudio-community\Qwen3-Coder-30B-A3B-Instruct-GGUF\Qwen3-Coder-30B-A3B-Instruct-Q4_K_M.gguf
      --jinja
      --temp 0.7
      --min-p 0.0
      --top-p 0.80
      --top-k 20
      --repeat-penalty 1.05
      --fit-ctx 60000

  Qwen3 Coder 30B A3B Instruct Q3_K_S ROCm:
    ttl: 900 # seconds to unload the model after use
    cmd: | # support for multi-line
      ${llamacpp-rocm}
      -m F:/ai/lmstudio-models/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF/Qwen3-Coder-30B-A3B-Instruct-Q3_K_S.gguf
      --jinja
      --temp 0.7
      --min-p 0.0
      --top-p 0.80
      --top-k 20
      --repeat-penalty 1.05

  Devstral Small 2 24B Instruct 2512 GGUF Q3_K_S ROCm:
    ttl: 900 # seconds to unload the model after use
    cmd: | # support for multi-line
      ${llamacpp-rocm}
      -m F:/ai/lmstudio-models/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF/Devstral-Small-2-24B-Instruct-2512-Q3_K_S.gguf
      --jinja
      --temp 0.15
      --min-p 0.01
      --fit-ctx 60000

  Nemotron 3 Nano 30B A3B Q4_K_M Vulkan:
    ttl: 900
    cmd: |
      ${llamacpp-vulkan}
      --temp 0.6
      --top-p 0.95
      -m F:/ai/lmstudio-models/lmstudio-community/NVIDIA-Nemotron-3-Nano-30B-A3B-GGUF/NVIDIA-Nemotron-3-Nano-30B-A3B-Q4_K_M.gguf

  Nemotron 3 Nano 30B A3B Q4_K_M ROCm:
    ttl: 900
    cmd: |
      ${llamacpp-rocm}
      --temp 0.6
      --top-p 0.95
      -m F:/ai/lmstudio-models/lmstudio-community/NVIDIA-Nemotron-3-Nano-30B-A3B-GGUF/NVIDIA-Nemotron-3-Nano-30B-A3B-Q4_K_M.gguf

  Devstral Small 2 24B Q4_K_XL Vulkan:
    ttl: 900
    cmd: |
      ${llamacpp-vulkan}
      -m F:\ai\lmstudio-models\unsloth\Devstral-Small-2-24B-Instruct-2512-GGUF\Devstral-Small-2-24B-Instruct-2512-UD-Q4_K_XL.gguf
      --jinja
      --min-p 0.01
      --temp 0.15
      --cpu-moe
      --fit-ctx 32000

  Devstral Small 2 24B Q4_K_XL ROCm:
    ttl: 900 # seconds to unload the model after use
    cmd: |
      ${llamacpp-rocm}
      -m F:\ai\lmstudio-models\unsloth\Devstral-Small-2-24B-Instruct-2512-GGUF\Devstral-Small-2-24B-Instruct-2512-UD-Q4_K_XL.gguf
      --jinja
      --min-p 0.01
      --temp 0.15
      --cpu-moe
      --fit-ctx 32000

  Qwen2.5-Coder-7B:
    ttl: 900 # seconds to unload the model after use
    cmd: |
      ${llamacpp-rocm}
      -m F:/ai/lmstudio-models/lmstudio-community/Qwen2.5-Coder-7B-Instruct-GGUF/Qwen2.5-Coder-7B-Instruct-Q4_K_M.gguf

  Qwen2.5-Coder-1.5B:
    ttl: 900 # seconds to unload the model after use
    cmd: |
      ${llamacpp-rocm}
      -m F:/ai/lmstudio-models/lmstudio-community/Qwen2.5-Coder-1.5B-Instruct-GGUF/Qwen2.5-Coder-1.5B-Instruct-Q8_0.gguf

  Ministral-3-14B-Reasoning-2512:
    ttl: 900 # seconds to unload the model after use
    cmd: |
      ${llamacpp-rocm}
      -m F:/ai/lmstudio-models/lmstudio-community/Ministral-3-14B-Reasoning-2512-GGUF/Ministral-3-14B-Reasoning-2512-Q4_K_M.gguf
      --mmproj F:\ai\lmstudio-models\lmstudio-community\Ministral-3-14B-Reasoning-2512-GGUF\mmproj-Ministral-3-14B-Reasoning-2512-F16.gguf

  whispercpp:
    ttl: 900 # seconds to unload the model after use
    cmd: | 
      C:\Users\Ismael\apps\whispercpp\whisper-server.exe
      --host 127.0.0.1
      --port ${PORT}
      -m C:\Users\Ismael\apps\whispercpp\models\ggml-small.bin
      --request-path /v1/audio/transcriptions
      --inference-path ""
      --convert
      -l auto
      -pp
      -pr
    checkEndpoint: /v1/audio/transcriptions/
    unlisted: true
    env:
      - "RESPONSE_FORMAT=TXT"

groups:
  group1:
    swap: false
    members:
      - "GPT-OSS 20B ROCm"
      - "Qwen2.5-Coder-1.5B"